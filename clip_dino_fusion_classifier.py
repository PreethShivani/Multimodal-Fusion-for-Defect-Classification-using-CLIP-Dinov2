# -*- coding: utf-8 -*-
"""FINAL DINO

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1QyxjaJIWDy2kECjpIOvFf1RHOk8ytY2E
"""

#@title Download the MVTec dataset sample from Supervisely
!curl https://assets.supervisely.com/supervisely-supervisely-assets-public/teams_storage/W/6/pt/ANAGVgKaC62tTrDQWK5JhNP2dd8ynqaTKSM1QdVoAasmTdaLvBwCuW7nCrq9o9lLS2padKnV9QogVGFlEPg7vxEBPIfuFC2Yq7ELNW7xn2t1egLrQPoGpNFJobhh.tar --output mvtec.tar

#@title Extract and list cable images
!tar -xf mvtec.tar
!ls test/img/cable_*

# Commented out IPython magic to ensure Python compatibility.
# Remove the existing repo folder if it’s half-broken
!rm -rf ZS-CLIP-AC-naive

# Clone again cleanly
!git clone https://github.com/akridata-ai/ZS-CLIP-AC-naive.git

# Move into the repo
# %cd ZS-CLIP-AC-naive

# Checkout the branch
!git checkout -b feature/template-code origin/feature/template-code

#@title Move cable images into repo's data folder
!cp /content/test/img/cable_* data/

#@title Install dependencies
!pip install -qr requirements.txt

# Commented out IPython magic to ensure Python compatibility.
# #@title Save cable-specific spec.py
# %%writefile spec.py
# """
# Spec containing the configuration for the defect classification task
# """
# from pydantic import BaseModel
# 
# 
# class DefectClassificationSpec(BaseModel):
#     model_name: str = "ViT-B/32"
#     use_dino: bool = True
#     fusion_mode: str = "concat"
#     alpha: float = 0.7
#     class_names: list[str] = [
#         "good",
#         "bent_wire",
#         "cable_swap",
#         "combined",
#         "cut_inner_insulation",
#         "cut_outer_insulation",
#         "missing_cable",
#         "missing_wire",
#         "poke_insulation"
#     ]
#     prompts: list[list[str]] = [
#         # good
#         [
#           "a photo of an intact electrical cable with no defects, no cuts, no missing wires, outer insulation smooth",
#           "an undamaged cable: correct wiring order, insulation unbroken, no bends or punctures",
#           "clean cable with proper connector mapping, no swapped wires, no damage",
#           "normal cable in good condition, all wires present, insulation intact, no visible anomalies",
#           "pristine cable: no cracks, no holes, no cuts, no miswiring",
#           "factory-new cable with correct pin order and continuous insulation"
#         ],
# 
#         #bent_wire
#         ["A cable with a wire bent out of shape",
#          "A cable showing bent or distorted wire"],
# 
#         # cable_swap
#         [
#           "a photo of a miswired cable: wires swapped at the connector, wrong pin order",
#           "cable with swapped connections: two wires crossed to the wrong terminals",
#           "connector with incorrect wiring sequence, wires interchanged (cable swap defect)",
#           "cable showing cross-connected leads, wrong color order at the plug",
#           "misconnection at the connector: wire positions exchanged, mapping incorrect",
#           "wires routed to wrong pins (swapped wiring) at the cable head",
#           "defect: swapped wires at connector, incorrect pinout"
#         ],
# 
#         # combined
#         [
#           "a photo of a cable with multiple defects at once (e.g., cut insulation and missing wire)",
#           "cable showing combined anomalies: miswiring plus damaged insulation",
#           "multiple simultaneous defects on one cable: missing conductor and outer cut",
#           "compound cable defect with both structural damage and wiring error",
#           "cable exhibiting more than one defect category in the same sample",
#           "mixed defects: punctured insulation together with swapped wiring",
#           "combined cable defect: two or more distinct problems visible"
#         ],
# 
#         #cut_inner_insulation
#         ["A cable with its inner insulation cut or sliced",
#          "Cable showing damage to the inner insulation layer"],
# 
#         #cut_outer_insulation
#         ["A cable with outer insulation cut",
#          "Cable showing slice or tear on the outer insulation"],
# 
#         #missing_cable
#         ["A cable missing an entire wire bundle",
#          "A cable with an absent main cable section"],
# 
#         #missing_wire
#         ["A cable missing a single wire",
#          "A cable with one or more wires removed"],
# 
#         # poke_insulation
#         [
#           "a cable with a small round hole punctured through the insulation",
#           "cable showing a sharp object poke defect in its insulation layer",
#           "wire insulation with a visible puncture or small localized hole",
#           "defect: insulation pierced but wire remains intact, not cut or missing",
#           "close-up of a cable with poke damage, tiny hole or indentation in the sheath",
#           "cable surface pierced with a sharp mark, unlike long cuts",
#           "small puncture defect in cable insulation, insulation broken at a point"
#         ]
# 
#   ]
#

# Commented out IPython magic to ensure Python compatibility.
# #@title Save updated clip_ac.py
# %%writefile clip_ac.py
# """
# Zero-shot defect classification using CLIP
# """
# 
# import clip
# import torch
# from PIL import Image
# from pathlib import Path
# from spec import DefectClassificationSpec
# import torchvision.transforms as T
# import torch.nn.functional as F
# 
# # Load DINOv2 globally (avoid reloading for every image)
# dino_model = torch.hub.load('facebookresearch/dinov2', 'dinov2_vits14')
# dino_model.eval()
# dino_preprocess = T.Compose([
#     T.Resize(224, interpolation=T.InterpolationMode.BICUBIC),
#     T.CenterCrop(224),
#     T.ToTensor(),
#     T.Normalize(mean=[0.485, 0.456, 0.406],
#                 std=[0.229, 0.224, 0.225]),
# ])
# 
# def classify_defects(test_dir, spec, return_paths=False):
#     device = "cuda" if torch.cuda.is_available() else "cpu"
#     model, preprocess = clip.load(spec.model_name, device=device)
# 
#     test_dir = Path(test_dir)
#     img_paths = sorted(test_dir.glob("cable_*.png"))   # cable dataset
# 
#     y_true, y_pred, file_paths = [], [], []
# 
#     # Precompute text features
#     text_features = []
#     with torch.no_grad():
#         for prompts in spec.prompts:
#             tokens = clip.tokenize(prompts).to(device)
#             encodings = model.encode_text(tokens)
#             encodings = encodings / encodings.norm(dim=-1, keepdim=True)
#             mean_encoding = encodings.mean(dim=0)
#             mean_encoding = mean_encoding / mean_encoding.norm()
#             text_features.append(mean_encoding)
#     text_features = torch.stack(text_features).to(device)
# 
#     # Iterate images
#     with torch.no_grad():
#         for img_path in img_paths:
#             label = next((cls for cls in spec.class_names if cls in img_path.stem), None)
#             if label is None:
#                 continue
#             img = Image.open(img_path).convert("RGB")
# 
#             # CLIP embedding
#             clip_input = preprocess(img).unsqueeze(0).to(device)
#             clip_features = model.encode_image(clip_input)
#             clip_features = clip_features / clip_features.norm(dim=-1, keepdim=True)
# 
#             # DINOv2 embedding
#             dino_input = dino_preprocess(img).unsqueeze(0)
#             dino_features = dino_model(dino_input).detach()
#             dino_features = dino_features / dino_features.norm(dim=-1, keepdim=True)
# 
#             # Fusion
#             if spec.use_dino and spec.fusion_mode == "weighted":
#                 sim_clip = (clip_features @ text_features.T)
#                 dino_confidence = torch.norm(dino_features).item()
#                 sim = spec.alpha * sim_clip + (1 - spec.alpha) * (dino_confidence * sim_clip)
# 
#             elif spec.use_dino and spec.fusion_mode == "concat":
#                 fused_features = torch.cat([clip_features, dino_features.to(device)], dim=-1)
#                 fused_features = F.normalize(fused_features, dim=-1)
# 
#                 expanded_text_features = []
#                 for tf in text_features:
#                     pad = torch.zeros(dino_features.shape[-1], device=device)
#                     expanded_text_features.append(torch.cat([tf, pad], dim=-1))
#                 expanded_text_features = torch.stack(expanded_text_features)
#                 expanded_text_features = F.normalize(expanded_text_features, dim=-1)
# 
#                 sim = fused_features @ expanded_text_features.T
# 
#             else:
#                 sim = (clip_features @ text_features.T)
# 
#             pred_idx = sim.argmax(dim=-1).item()
#             pred_label = spec.class_names[pred_idx]
# 
#             y_true.append(label)
#             y_pred.append(pred_label)
#             file_paths.append(str(img_path))
# 
#     if return_paths:
#         return y_true, y_pred, file_paths
#     else:
#         return y_true, y_pred
#

!pip install -q git+https://github.com/openai/CLIP.git

# -----------------------------
# CLIP + DINOv2 Fusion Features + Logistic Regression (with Cross-validation)
# -----------------------------

from spec import DefectClassificationSpec
from clip_ac import dino_model, dino_preprocess
import clip
import numpy as np
import torch
from pathlib import Path
from PIL import Image

device = "cuda" if torch.cuda.is_available() else "cpu"
clip_model, clip_preprocess = clip.load("ViT-B/32", device=device)

def extract_clip_dino_embeddings(test_dir, spec):
    test_dir = Path(test_dir)
    img_paths = sorted(test_dir.glob("cable_*.png"))

    X, y, paths = [], [], []
    with torch.no_grad():
        for img_path in img_paths:
            # robust label parsing: match the full class name in the filename
            label = next((cls for cls in spec.class_names if cls in img_path.stem), None)
            if label is None:
                continue

            img = Image.open(img_path).convert("RGB")

            # CLIP embedding
            clip_input = clip_preprocess(img).unsqueeze(0).to(device)
            clip_features = clip_model.encode_image(clip_input)
            clip_features = clip_features / clip_features.norm(dim=-1, keepdim=True)
            clip_features = clip_features.cpu().numpy().flatten()

            # DINO embedding
            dino_input = dino_preprocess(img).unsqueeze(0)
            dino_features = dino_model(dino_input).detach()
            dino_features = dino_features / dino_features.norm(dim=-1, keepdim=True)
            dino_features = dino_features.cpu().numpy().flatten()

            # Fusion
            fused_features = np.concatenate([clip_features, dino_features])

            X.append(fused_features)
            y.append(label)
            paths.append(str(img_path))

    return np.array(X), np.array(y), paths


# ---- Train Logistic Regression on fused features ----
from sklearn.pipeline import make_pipeline
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay
from sklearn.model_selection import cross_val_predict, StratifiedKFold
import matplotlib.pyplot as plt

spec = DefectClassificationSpec()
X_fused, y_fused, paths_fused = extract_clip_dino_embeddings("data", spec)

# scaler + multinomial LR tends to work best here
clf_fused = make_pipeline(
    StandardScaler(with_mean=True, with_std=True),
    LogisticRegression(
        max_iter=2000,
        multi_class="multinomial",
        solver="lbfgs",
        n_jobs=None
    )
)

# use reproducible, stratified CV (min class size >= 3 → cv=3 is valid)
cv = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)
y_pred_fused = cross_val_predict(clf_fused, X_fused, y_fused, cv=cv)

print("CLIP + DINOv2 Logistic Regression Report (3-fold CV):")
print(classification_report(y_fused, y_pred_fused, zero_division=0))

cm = confusion_matrix(y_fused, y_pred_fused, labels=spec.class_names)
fig, ax = plt.subplots(figsize=(12, 10))
disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=spec.class_names)
disp.plot(cmap="Purples", ax=ax, xticks_rotation=45, colorbar=False)
ax.set_title("Confusion Matrix — CLIP + DINOv2 + Logistic Regression (3-fold CV)")
plt.tight_layout()
plt.show()





